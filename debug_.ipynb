{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10, 10, 10, 10, 10, 10, 10, 10],\n",
       "        [10, 10, 10, 10, 10, 10, 10, 10]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "#现在有batch size 2，每张图片8个query\n",
    "#比如数据集有9种类别 现在+1 no obj\n",
    "# 类别从1开始   1 2 3 4 5 6 7 8 9\n",
    "num_classes = 10\n",
    "target_classes = torch.full([2,8], num_classes, dtype=torch.int64)\n",
    "\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 8])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10,  1, 10, 10, 10, 10,  7, 10],\n",
       "        [10, 10, 10,  6, 10, 10, 10, 10]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes[0][1] = 1\n",
    "target_classes[0][6] = 7\n",
    "target_classes[1][3] = 6\n",
    "target_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes_onehot = torch.zeros([2, 8, num_classes+1], dtype=torch.int64)\n",
    "target_classes_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes_onehot.scatter_(2, target_classes.unsqueeze(-1), 1)\n",
    "target_classes_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 1, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "\n",
       "        [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_classes_onehot = target_classes_onehot[:,:,:-1]\n",
    "target_classes_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test里面总共有152226关系\n",
      "主语宾语class一样的关系0\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "import json\n",
    "from collections import defaultdict\n",
    "# 路径到 rel.json 和 val.json 文件\n",
    "path_to_rel_json = \"/home/cmh/cmh/projects/detrs/RelTR/data/vg/rel.json\"\n",
    "path_to_val_json = \"/home/cmh/cmh/projects/detrs/RelTR/data/vg/test.json\"\n",
    "\n",
    "# 加载 rel.json 文件\n",
    "with open(path_to_rel_json) as f:\n",
    "    rel_data = json.load(f)\n",
    "\n",
    "# 加载 val.json 文件\n",
    "with open(path_to_val_json) as f:\n",
    "    val_data = json.load(f)\n",
    "\n",
    "# 提取 val 中的图像和对象信息\n",
    "# val_images = {img[\"id\"]: img for img in val_data[\"images\"]}\n",
    "# val_annotations = {}\n",
    "# for ann in val_data[\"annotations\"]:\n",
    "#     image_id = ann[\"image_id\"]\n",
    "#     if image_id not in val_annotations.keys():\n",
    "#         val_annotations[image_id] = []\n",
    "#     val_annotations[image_id].append(ann)\n",
    "\n",
    "val_annotations = defaultdict(list)\n",
    "# for ann in val_data[\"annotations\"]:\n",
    "#     image_id = ann[\"image_id\"]\n",
    "#     if image_id not in val_annotations:\n",
    "#         val_annotations[image_id] = []\n",
    "#     val_annotations[image_id].append(ann)\n",
    "\n",
    "for ann in val_data[\"annotations\"]:\n",
    "    val_annotations[ann[\"image_id\"]].append(ann)\n",
    "\n",
    "val_annotations[1]\n",
    "# 检查 rel.json 中的三元组是否有主语和宾语类别相同的情况\n",
    "same_category_pairs = []\n",
    "rel_len_sum = 0\n",
    "for img_id, triplets in rel_data[\"test\"].items():\n",
    "    # print(f'img_id {img_id}')\n",
    "    # print(len(val_annotations[int(img_id)]))\n",
    "\n",
    "    temp_len = len(triplets)\n",
    "    rel_len_sum += temp_len  \n",
    "    for triplet in triplets:\n",
    "        subj_id, obj_id, _ = triplet\n",
    "        subj_category = val_annotations[int(img_id)][subj_id]\n",
    "        obj_category = val_annotations[int(img_id)][obj_id]\n",
    "\n",
    "        if subj_category == obj_category:\n",
    "            same_category_pairs.append((img_id, subj_id, obj_id, subj_category))\n",
    "\n",
    "# 输出结果\n",
    "num_same_category_pairs = len(same_category_pairs)\n",
    "# same_category_pairs[:5], num_same_category_pairs  # 显示前5个结果和总数\n",
    "num_same_category_pairs\n",
    "\n",
    "print(f'test里面总共有{rel_len_sum}关系')\n",
    "print(f'主语宾语class一样的关系{num_same_category_pairs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里寻找合适的mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "num_queries = 5\n",
    "single_pad = 3\n",
    "scalar = 2\n",
    "pad_size = single_pad * scalar\n",
    "\n",
    "tgt_size = pad_size + num_queries \n",
    "\n",
    "attn_mask = torch.ones(tgt_size, tgt_size).to('cuda') < 0   # [390,390]\n",
    "# match query cannot see the reconstruct\n",
    "attn_mask[pad_size:, :pad_size] = True        #300行90列的长方形设置为True, 目的是让300个query和90个dn query attn_mask为True\n",
    "# reconstruct cannot see each other\n",
    "for i in range(scalar):\n",
    "    if i == 0:\n",
    "        # attn_mask[0:18, 18:90]\n",
    "        attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n",
    "    if i == scalar - 1:\n",
    "        # attn_mask[18*4:18*5, 0:18]\n",
    "        attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n",
    "    else:\n",
    "        attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n",
    "        attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n",
    "\n",
    "attn_mask.size()\n",
    "attn_mask = attn_mask.long()\n",
    "attn_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 18])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_attn_mask(num_subject_queries, num_object_queries, num_denoising_groups, single_pad):\n",
    "    total_queries = num_subject_queries + num_object_queries  # Total number of queries for both subject and object\n",
    "    pad_size = num_denoising_groups * single_pad # Total denoising queries for each of subject and object\n",
    "    mask_size = total_queries + pad_size * 2  # Total size of the mask, including both subject and object denoising groups\n",
    "\n",
    "    # Initialize mask to False (allowing attention interaction)\n",
    "    attn_mask = torch.zeros((mask_size, mask_size), dtype=torch.bool)\n",
    "\n",
    "    # Subject matching query not interacting with own and object's denoising group\n",
    "    attn_mask[pad_size : (pad_size + num_subject_queries), : pad_size] = True \n",
    "    attn_mask[pad_size : (pad_size + num_subject_queries), (num_subject_queries + pad_size) : (num_subject_queries+ pad_size + pad_size)] = True \n",
    "\n",
    "    # Object matching query not interacting with subject's and own denoising group\n",
    "    attn_mask[(num_subject_queries + 2*pad_size) : mask_size, : pad_size] = True\n",
    "    attn_mask[(num_subject_queries + 2*pad_size) : mask_size, (num_subject_queries + pad_size) : (num_subject_queries+ pad_size + pad_size)] = True\n",
    "\n",
    "    # Denoising group interactions\n",
    "    for i in range(num_denoising_groups):\n",
    "        if i == 0:\n",
    "            # attn_mask[0:18, 18:90]\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), (pad_size + num_subject_queries + single_pad * (i + 1)) : (num_subject_queries + 2*pad_size)] = True\n",
    "            attn_mask[(pad_size + num_subject_queries):(pad_size + num_subject_queries + single_pad * (i + 1)), single_pad * (i + 1):pad_size] = True\n",
    "            attn_mask[(pad_size + num_subject_queries):(pad_size + num_subject_queries + single_pad * (i + 1)), (pad_size + num_subject_queries + single_pad * (i + 1)) : (num_subject_queries + 2*pad_size)] = True\n",
    "        if i == num_denoising_groups - 1:\n",
    "            # attn_mask[18*4:18*5, 0:18]\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n",
    "            #行不变，列变\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), (pad_size + num_subject_queries):(pad_size + num_subject_queries+ single_pad * i)] = True\n",
    "            #行变，列不变\n",
    "            attn_mask[(pad_size + num_subject_queries + single_pad * i):(pad_size + num_subject_queries + single_pad * i)+single_pad, :single_pad * i] = True\n",
    "            #行变，列变\n",
    "            attn_mask[(pad_size + num_subject_queries + single_pad * i):(pad_size + num_subject_queries + single_pad * i)+single_pad, (pad_size + num_subject_queries):(pad_size + num_subject_queries+ single_pad * i)] = True\n",
    "\n",
    "        else:\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), :single_pad * i] = True\n",
    "\n",
    "            #行不变，列变\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), (pad_size + num_subject_queries + single_pad * (i + 1)) : (2*pad_size + num_subject_queries)] = True\n",
    "            attn_mask[single_pad * i:single_pad * (i + 1), (pad_size + num_subject_queries) : (pad_size + num_subject_queries + single_pad * i)] = True\n",
    "\n",
    "            #行变，列不变\n",
    "            attn_mask[pad_size + num_subject_queries+single_pad * i:pad_size + num_subject_queries+single_pad * (i + 1), single_pad * (i + 1):pad_size] = True\n",
    "            attn_mask[pad_size + num_subject_queries+single_pad * i:pad_size + num_subject_queries+single_pad * (i + 1), :single_pad * i] = True\n",
    "\n",
    "            #行变，列变\n",
    "            attn_mask[pad_size + num_subject_queries+single_pad * i:pad_size + num_subject_queries+single_pad * (i + 1), (pad_size + num_subject_queries + single_pad * (i + 1)) : (2*pad_size + num_subject_queries)] = True\n",
    "            attn_mask[pad_size + num_subject_queries+single_pad * i:pad_size + num_subject_queries+single_pad * (i + 1), (pad_size + num_subject_queries) : (pad_size + num_subject_queries + single_pad * i)] = True\n",
    "            \n",
    "    return attn_mask\n",
    "\n",
    "attn_mask2 = create_attn_mask(3, 3, single_pad=2, num_denoising_groups=3)\n",
    "print(attn_mask2.size())\n",
    "attn_mask2.long()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "exia-detr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
